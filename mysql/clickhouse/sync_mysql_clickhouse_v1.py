#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MySQLåˆ°ClickHouseé«˜é€ŸåŒæ­¥å·¥å…· - æ€§èƒ½ä¼˜åŒ–ç‰ˆ

åŠŸèƒ½ç‰¹æ€§ï¼š
    âš¡ æé€Ÿå¢é‡åŒæ­¥ï¼ˆæ™ºèƒ½æ£€æµ‹æœ‰æ— æ–°æ•°æ®ï¼‰
    ğŸ”„ é˜²å¹¶å‘é—æ¼ï¼ˆæ¯5åˆ†é’Ÿå›é€€æ£€æŸ¥ï¼‰
    ğŸš€ é›¶å¼€é”€æ£€æµ‹ï¼Œåªåœ¨æœ‰æ•°æ®æ—¶åŒæ­¥
    ğŸ• å®Œç¾å…¼å®¹DateTime/DateTime64
    ğŸ§  è‡ªåŠ¨å¤„ç†æ—¶åŒºå’Œç²¾åº¦é—®é¢˜

ä½¿ç”¨ç¤ºä¾‹ï¼š
    python sync_mysql_clickhouse.py
    python sync_mysql_clickhouse.py --config /path/to/config.json
    python sync_mysql_clickhouse.py --reset --force

é…ç½®æ–‡ä»¶æ ¼å¼ (config.json)ï¼š
    {
      "clickhouse": {
        "host": "localhost",
        "port": 8123,
        "username": "default",
        "password": "your_password",
        "database": "your_database"
      },
      "mysql": {
        "host": "172.17.0.1",
        "port": 61786,
        "database": "your_database",
        "username": "your_username",
        "password": "your_password"
      },
      "tables": {
        "table_name": {
          "id_field": "id",
          "time_field": "update_time"
        }
      }
    }
å­—æ®µè¯´æ˜ï¼š
    ğŸ“‹ id_field: ä¸»é”®å­—æ®µï¼Œç”¨äºæ’åºå’Œå»é‡
        - æ”¯æŒ: id, user_id, order_id, uuid, ç­‰ä»»æ„å”¯ä¸€å­—æ®µ
        - å»ºè®®: ä½¿ç”¨æ•°å€¼å‹ä¸»é”®ä»¥è·å¾—æœ€ä½³æ€§èƒ½

    â° time_field: æ—¶é—´å­—æ®µï¼Œç”¨äºå¢é‡åŒæ­¥
        - æ”¯æŒ: created_at, updated_at, modify_time, timestamp, ç­‰
        - ç±»å‹: DateTime, DateTime64, TIMESTAMP éƒ½å®Œç¾å…¼å®¹
        - å»ºè®®: ä½¿ç”¨ updated_at å­—æ®µä»¥æ•è·æ‰€æœ‰æ•°æ®å˜æ›´

æ€§èƒ½å»ºè®®ï¼š
    ğŸš€ MySQLç«¯ä¼˜åŒ–:
        - åœ¨ time_field ä¸Šåˆ›å»ºç´¢å¼•: CREATE INDEX idx_update_time ON table_name(update_time)
        - ä½¿ç”¨åªè¯»ç”¨æˆ·ï¼Œé¿å…å½±å“ç”Ÿäº§ç¯å¢ƒ
        - è€ƒè™‘ä½¿ç”¨MySQLä»åº“è¿›è¡ŒåŒæ­¥

    âš¡ ClickHouseç«¯ä¼˜åŒ–:
        - é€‰æ‹©åˆé€‚çš„ ORDER BY å­—æ®µï¼ˆé€šå¸¸æ˜¯ä¸»é”®ï¼‰
        - ä½¿ç”¨ ReplacingMergeTree å¼•æ“è‡ªåŠ¨å»é‡
        - å®šæœŸæ‰§è¡Œ OPTIMIZE TABLE åˆå¹¶æ•°æ®å—

"""


import clickhouse_connect
import time
import threading
import logging
import signal
import sys
import argparse
import json
import os

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')

def load_config(config_path):
    if not os.path.exists(config_path):
        example_config = {
            "clickhouse": {"host": "localhost", "port": 8123, "username": "default", "password": "your_password", "database": "your_database"},
            "mysql": {"host": "172.17.0.1", "port": 61786, "database": "your_database", "username": "your_username", "password": "your_password"},
            "tables": {"crm_item_main": {"id_field": "id", "time_field": "update_time"}}
        }
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(example_config, f, indent=4, ensure_ascii=False)
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå·²åˆ›å»ºç¤ºä¾‹é…ç½®æ–‡ä»¶: {config_path}")
        sys.exit(1)

    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        for key in ['clickhouse', 'mysql', 'tables']:
            if key not in config:
                raise KeyError(f"é…ç½®æ–‡ä»¶ç¼ºå°‘å¿…è¦é¡¹: {key}")
        logging.info(f"âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {config_path}")
        return config
    except Exception as e:
        print(f"âŒ åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        sys.exit(1)

def smart_mysql_to_clickhouse_sync():
    parser = argparse.ArgumentParser(description='MySQLåˆ°ClickHouseé«˜é€ŸåŒæ­¥å·¥å…·')
    parser.add_argument('--config', default='config.json', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--reset', action='store_true', help='é‡ç½®è¡¨')
    parser.add_argument('--force', action='store_true', help='å¼ºåˆ¶æ‰§è¡Œ')
    args = parser.parse_args()

    config = load_config(args.config)
    CLICKHOUSE_CONFIG = config['clickhouse']
    MYSQL_CONFIG = config['mysql']
    TABLES = config['tables']

    stop_event = threading.Event()
    last_backtrack_time = {}

    def create_client():
        return clickhouse_connect.get_client(**CLICKHOUSE_CONFIG)

    def reset_tables(client):
        if not args.force:
            print("âš ï¸  è­¦å‘Šï¼šè¿™å°†åˆ é™¤æ‰€æœ‰ç°æœ‰æ•°æ®ï¼")
            if input("ç¡®è®¤é‡ç½®ï¼Ÿ(è¾“å…¥ 'yes' ç¡®è®¤): ").lower() != 'yes':
                return False

        for table_name in TABLES.keys():
            try:
                client.command(f"DROP TABLE IF EXISTS {table_name}")
                logging.info(f"ğŸ—‘ï¸  å·²åˆ é™¤è¡¨: {table_name}")
            except Exception as e:
                logging.warning(f"âš ï¸  åˆ é™¤è¡¨å¤±è´¥: {e}")

        try:
            client.command("DROP DATABASE IF EXISTS mysql_db")
            logging.info("ğŸ—‘ï¸  å·²åˆ é™¤MySQLæ•°æ®åº“å¼•æ“")
        except:
            pass
        return True

    def init_setup():
        # åˆ›å»ºæ•°æ®åº“
        temp_config = CLICKHOUSE_CONFIG.copy()
        temp_config['database'] = 'default'
        temp_client = clickhouse_connect.get_client(**temp_config)
        temp_client.command(f"CREATE DATABASE IF NOT EXISTS {CLICKHOUSE_CONFIG['database']}")
        temp_client.close()

        client = create_client()

        if args.reset and not reset_tables(client):
            client.close()
            return False

        # åˆ›å»ºMySQLå¼•æ“
        mysql_db_sql = f"""CREATE DATABASE IF NOT EXISTS mysql_db ENGINE = MySQL('{MYSQL_CONFIG['host']}:{MYSQL_CONFIG['port']}', '{MYSQL_CONFIG['database']}', '{MYSQL_CONFIG['username']}', '{MYSQL_CONFIG['password']}')"""
        client.command(mysql_db_sql)

        # åˆ›å»ºè¡¨
        for table_name, table_config in TABLES.items():
            if not client.query(f"EXISTS TABLE {table_name}").result_rows[0][0]:
                create_sql = f"""CREATE TABLE {table_name} ENGINE = ReplacingMergeTree({table_config['time_field']}) ORDER BY {table_config['id_field']} AS SELECT * FROM mysql_db.{table_name} LIMIT 0"""
                client.command(create_sql)

                # è½¬æ¢DateTimeå­—æ®µä¸ºDateTime64(3)
                try:
                    columns = client.query(f"DESCRIBE {table_name}").result_rows
                    for col_name, col_type, *_ in columns:
                        if 'DateTime' in col_type and 'DateTime64' not in col_type:
                            client.command(f"ALTER TABLE {table_name} MODIFY COLUMN {col_name} DateTime64(3)")
                    logging.info(f"âœ… è¡¨åˆ›å»ºæˆåŠŸ: {table_name}")
                except Exception as e:
                    logging.warning(f"âš ï¸  å­—æ®µè½¬æ¢å¤±è´¥: {e}")
                    logging.info(f"âœ… è¡¨åˆ›å»ºæˆåŠŸ: {table_name}")
            else:
                logging.info(f"â„¹ï¸  è¡¨å·²å­˜åœ¨: {table_name}")

            # åˆå§‹åŒ–å›é€€æ—¶é—´
            last_backtrack_time[table_name] = time.time()

        client.close()
        logging.info("âœ… åˆå§‹åŒ–å®Œæˆ")
        return True

    def get_max_timestamp_with_backtrack(client, table_name, time_field):
        """ğŸ§  æ™ºèƒ½è·å–æœ€å¤§æ—¶é—´æˆ³ï¼Œå®Œç¾å…¼å®¹æ‰€æœ‰DateTimeç±»å‹"""
        try:
            # ğŸš€ ä½¿ç”¨toUnixTimestampç»Ÿä¸€å¤„ç†ï¼Œå…¼å®¹DateTimeå’ŒDateTime64
            result = client.query(f"SELECT toUnixTimestamp(MAX({time_field})) FROM {table_name} FINAL")
            max_timestamp = result.result_rows[0][0]

            if not max_timestamp or max_timestamp == 0:
                return 0, False

            current_time = time.time()
            # ğŸ”„ æ”¹ä¸ºæ¯5åˆ†é’Ÿå›é€€ä¸€æ¬¡
            should_backtrack = current_time - last_backtrack_time.get(table_name, 0) >= 300  # 5åˆ†é’Ÿ = 300ç§’

            if should_backtrack:
                # ğŸ• å›é€€åˆ°5åˆ†é’Ÿå‰çš„æ—¶é—´ç‚¹ï¼Œç¡®ä¿ä¸é—æ¼æ•°æ®
                backtrack_timestamp = current_time - 300  # å½“å‰æ—¶é—´å¾€å‰5åˆ†é’Ÿ
                last_backtrack_time[table_name] = current_time
                return backtrack_timestamp, True
            else:
                return max_timestamp, False

        except Exception as e:
            logging.warning(f"âš ï¸  è·å–æ—¶é—´æˆ³å¤±è´¥ {table_name}: {e}")
            return 0, False

    def sync_data_ultra_fast(client, table_name, table_config):
        """âš¡ è¶…é«˜é€ŸåŒæ­¥ï¼Œå…ˆæ£€æµ‹å†åŒæ­¥"""
        if stop_event.is_set():
            return False

        time_field = table_config['time_field']

        try:
            max_timestamp, is_backtrack = get_max_timestamp_with_backtrack(client, table_name, time_field)

            # ğŸ” å…ˆæ£€æµ‹æ˜¯å¦æœ‰æ–°æ•°æ®ï¼ˆæè½»é‡çº§æŸ¥è¯¢ï¼‰
            check_sql = f"SELECT COUNT(*) FROM mysql_db.{table_name} WHERE toUnixTimestamp({time_field}) > {int(max_timestamp)} LIMIT 1"
            check_result = client.query(check_sql)
            new_count = check_result.result_rows[0][0]

            # ğŸš€ åªæœ‰ç¡®å®æœ‰æ–°æ•°æ®æ—¶æ‰æ‰§è¡ŒåŒæ­¥
            if new_count > 0:
                sync_sql = f"INSERT INTO {table_name} SELECT * FROM mysql_db.{table_name} WHERE toUnixTimestamp({time_field}) > {int(max_timestamp)}"
                client.command(sync_sql)

                if is_backtrack:
                    logging.info(f"ğŸ”„ {table_name}: 5åˆ†é’Ÿå›é€€æ£€æŸ¥å®Œæˆï¼ŒåŒæ­¥äº† {new_count} æ¡æ•°æ®")
                else:
                    logging.info(f"âš¡ {table_name}: åŒæ­¥äº† {new_count} æ¡æ–°æ•°æ®")
                return True
            else:
                # ğŸ“Š é™é»˜æ¨¡å¼ï¼šæ— æ–°æ•°æ®æ—¶ä¸è¾“å‡ºæ—¥å¿—ï¼Œé¿å…åˆ·å±
                return False

        except Exception as e:
            if not stop_event.is_set():
                logging.error(f"âŒ {table_name} åŒæ­¥å¤±è´¥: {e}")
            return False

    def incremental_sync():
        client = create_client()
        logging.info("ğŸš€ è¶…é«˜é€ŸåŒæ­¥å¯åŠ¨ï¼ˆæ™ºèƒ½æ£€æµ‹æ¨¡å¼ï¼‰")

        try:
            while not stop_event.is_set():
                for table_name, table_config in TABLES.items():
                    if stop_event.is_set():
                        break
                    sync_data_ultra_fast(client, table_name, table_config)

                if stop_event.is_set():
                    break

                # â±ï¸ å›ºå®š0.5ç§’è½®è¯¢é—´éš”
                stop_event.wait(0.5)

        except Exception as e:
            if not stop_event.is_set():
                logging.error(f"âŒ åŒæ­¥å¼‚å¸¸: {e}")
        finally:
            client.close()
            logging.info("ğŸ›‘ åŒæ­¥å·²åœæ­¢")

    def signal_handler(signum, frame):
        logging.info("ğŸ›‘ æ­£åœ¨é€€å‡º...")
        stop_event.set()

    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)

    try:
        if not init_setup():
            return

        if args.reset:
            logging.info("ğŸ”„ é‡ç½®å®Œæˆ")
            return

        inc_thread = threading.Thread(target=incremental_sync)
        inc_thread.start()

        logging.info("ğŸš€ è¶…é«˜é€ŸåŒæ­¥å·²å¯åŠ¨ï¼ˆæ™ºèƒ½æ£€æµ‹æ¨¡å¼ï¼‰")
        logging.info(f"ğŸ“‹ ç›‘æ§è¡¨: {', '.join(TABLES.keys())}")
        logging.info("ğŸ”„ æ¯5åˆ†é’Ÿè‡ªåŠ¨å›é€€æ£€æŸ¥")
        logging.info("ğŸ§  æ™ºèƒ½å…¼å®¹DateTime/DateTime64")
        logging.info("âš¡ å…ˆæ£€æµ‹ååŒæ­¥ï¼Œé›¶æ— æ•ˆå¼€é”€")
        logging.info("ğŸ“Š é™é»˜æ¨¡å¼ï¼šæ— æ–°æ•°æ®æ—¶ä¸è¾“å‡ºæ—¥å¿—")
        logging.info("â±ï¸ å›ºå®š0.5ç§’è½®è¯¢é—´éš”")
        logging.info("ğŸ’¡ æŒ‰ Ctrl+C é€€å‡º")

        while not stop_event.is_set():
            stop_event.wait(1)

        inc_thread.join(timeout=5)
        logging.info("ğŸ›‘ å·²åœæ­¢")

    except Exception as e:
        logging.error(f"âŒ å¼‚å¸¸: {e}")
        stop_event.set()
    finally:
        sys.exit(0)

if __name__ == "__main__":
    smart_mysql_to_clickhouse_sync()

